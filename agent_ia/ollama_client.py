"""
Client pour communiquer avec Ollama.
"""
import requests
import json
from typing import Dict, Any, Optional
from .utils.code_cleaner import clean_and_validate
from .config import OLLAMA_BASE_URL, OLLAMA_MODEL, OLLAMA_TIMEOUT, GENERATION_CONFIG


class OllamaClient:
    """
    Client pour interagir avec le serveur Ollama local.
    """
    
    def __init__(self, model: str = OLLAMA_MODEL):
        """
        Initialiser le client.
        
        Args:
            model: Nom du mod√®le Ollama √† utiliser
        """
        self.base_url = OLLAMA_BASE_URL
        self.model = model
        self.timeout = OLLAMA_TIMEOUT
    
    def generate(self, prompt: str, **kwargs) -> str:
        """
        G√©n√©rer du texte √† partir d'un prompt.
        
        Args:
            prompt: Le prompt √† envoyer au mod√®le
            **kwargs: Param√®tres suppl√©mentaires (temperature, top_p, etc.)
        
        Returns:
            str: Le texte g√©n√©r√©
        
        Raises:
            ConnectionError: Si Ollama n'est pas accessible
            ValueError: Si la r√©ponse est invalide
        """
        # Fusionner les param√®tres par d√©faut avec ceux fournis
        config = {**GENERATION_CONFIG, **kwargs}
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,  # R√©ponse compl√®te
            "options": config
        }
        
        try:
            print(f"ü§ñ G√©n√©ration avec {self.model}...")
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=self.timeout
            )
            
            response.raise_for_status()  # L√®ve une exception si erreur HTTP
            
            result = response.json()
            generated_text = result.get('response', '')
            
            if not generated_text:
                raise ValueError("R√©ponse vide du mod√®le")
            
            print(f"‚úÖ G√©n√©ration termin√©e ({len(generated_text)} caract√®res)")
            
            return generated_text
        
        except requests.exceptions.ConnectionError:
            raise ConnectionError(
                f"‚ùå Impossible de se connecter √† Ollama sur {self.base_url}. "
                "V√©rifiez qu'Ollama est bien lanc√© (commande: ollama serve)"
            )
        
        except requests.exceptions.Timeout:
            raise TimeoutError(
                f"‚è±Ô∏è  La g√©n√©ration a pris plus de {self.timeout}s. "
                "Essayez de simplifier le prompt ou d'augmenter le timeout."
            )
        
        except requests.exceptions.HTTPError as e:
            raise ValueError(
                f"‚ùå Erreur HTTP {e.response.status_code}: {e.response.text}"
            )
            
    def generate_dag_code(self, prompt: str, **kwargs) -> tuple[str, bool, str]:
        """
        G√©n√©rer du code DAG et le nettoyer automatiquement.
        
        Version sp√©cialis√©e de generate() qui :
        - G√©n√®re le code
        - Nettoie le markdown et les explications
        - Valide la structure
        
        Args:
            prompt: Prompt de g√©n√©ration
            **kwargs: Param√®tres de g√©n√©ration
        
        Returns:
            tuple[str, bool, str]: (code_nettoy√©, is_valid, error_message)
        
        Example:
            >>> client = OllamaClient()
            >>> code, valid, error = client.generate_dag_code(prompt)
            >>> if valid:
            ...     print("Code pr√™t √† sauvegarder")
        """
        # G√©n√©rer le code brut
        raw_code = self.generate(prompt, **kwargs)
        
        # Nettoyer et valider
        clean_code, is_valid, error_msg = clean_and_validate(raw_code)
        
        if not is_valid:
            print(f"‚ö†Ô∏è  Code g√©n√©r√© invalide : {error_msg}")
            print("üí° Le code sera quand m√™me retourn√© pour correction manuelle")
        
        return clean_code, is_valid, error_msg

    
    def chat(self, messages: list, **kwargs) -> str:
        """
        Converser avec le mod√®le (mode chat).
        
        Args:
            messages: Liste de messages [{"role": "user", "content": "..."}]
            **kwargs: Param√®tres suppl√©mentaires
        
        Returns:
            str: La r√©ponse du mod√®le
        """
        config = {**GENERATION_CONFIG, **kwargs}
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": config
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            result = response.json()
            
            return result['message']['content']
        
        except Exception as e:
            raise Exception(f"Erreur lors du chat: {str(e)}")
    
    def list_models(self) -> list:
        """
        Lister les mod√®les disponibles.
        
        Returns:
            list: Liste des mod√®les install√©s
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            
            models = response.json().get('models', [])
            return [model['name'] for model in models]
        
        except Exception as e:
            raise Exception(f"Erreur lors de la r√©cup√©ration des mod√®les: {str(e)}")
    
    def is_available(self) -> bool:
        """
        V√©rifier si Ollama est accessible.
        
        Returns:
            bool: True si Ollama r√©pond
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False


# === Fonction utilitaire ===

def get_client(model: Optional[str] = None) -> OllamaClient:
    """
    Factory pour cr√©er un client Ollama.
    
    Args:
        model: Mod√®le √† utiliser (optionnel)
    
    Returns:
        OllamaClient: Instance du client
    """
    return OllamaClient(model=model or OLLAMA_MODEL)
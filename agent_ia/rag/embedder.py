"""
Module pour crÃ©er des embeddings (reprÃ©sentations vectorielles de texte).

Un embedding transforme du texte en vecteur numÃ©rique qui capture le sens sÃ©mantique.
Exemple : "chat" et "cat" auront des vecteurs trÃ¨s proches.
"""

from sentence_transformers import SentenceTransformer
from typing import List, Union
import numpy as np


class Embedder:
    """
    Classe pour gÃ©rer la crÃ©ation d'embeddings.
    
    Utilise le modÃ¨le 'all-MiniLM-L6-v2' qui :
    - Produit des vecteurs de 384 dimensions
    - Est optimisÃ© pour la recherche sÃ©mantique
    - Fonctionne bien en anglais
    - Est lÃ©ger (80 MB) et rapide
    """
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Initialiser l'embedder avec un modÃ¨le spÃ©cifique.
        
        Args:
            model_name (str): Nom du modÃ¨le SentenceTransformer.
                             Par dÃ©faut : 'all-MiniLM-L6-v2'
        
        Note:
            Au premier lancement, le modÃ¨le sera tÃ©lÃ©chargÃ© depuis
            Hugging Face (~80 MB). Les fois suivantes, il utilisera
            la version en cache.
        """
        print(f"ğŸ“¦ Chargement du modÃ¨le d'embedding : {model_name}")
        print("   (Peut prendre 30s au premier lancement...)")
        
        # Charger le modÃ¨le
        # device=None â†’ Utilise GPU si disponible, sinon CPU
        self.model = SentenceTransformer(model_name, device=None)
        self.model_name = model_name
        self.embedding_dimension = self.model.get_sentence_embedding_dimension()
        
        print(f"âœ… ModÃ¨le chargÃ© : {self.embedding_dimension} dimensions")
    
    def encode(self, texts: Union[str, List[str]], 
               show_progress: bool = False) -> np.ndarray:
        """
        Encoder un ou plusieurs textes en embeddings.
        
        Args:
            texts: Un texte (str) ou une liste de textes (List[str])
            show_progress: Afficher une barre de progression (dÃ©faut: False)
        
        Returns:
            np.ndarray: 
                - Si input = str  â†’ shape (embedding_dim,)
                - Si input = list â†’ shape (n_texts, embedding_dim)
        
        Example:
            >>> embedder = Embedder()
            >>> 
            >>> # Un seul texte
            >>> emb = embedder.encode("Hello world")
            >>> emb.shape
            (384,)
            >>> 
            >>> # Plusieurs textes
            >>> embs = embedder.encode(["Hello", "World"])
            >>> embs.shape
            (2, 384)
        """
        # Convertir str en list si nÃ©cessaire
        was_single_string = isinstance(texts, str)
        if was_single_string:
            texts = [texts]
        
        # Encoder les textes
        embeddings = self.model.encode(
            texts,
            show_progress_bar=show_progress,
            convert_to_numpy=True,  # Retourner des numpy arrays
            normalize_embeddings=True  # Normaliser pour similaritÃ© cosinus
        )
        
        # Si input Ã©tait un seul string, retourner un seul vecteur
        if was_single_string:
            return embeddings[0]
        
        return embeddings
    
    def similarity(self, text1: str, text2: str) -> float:
        """
        Calculer la similaritÃ© sÃ©mantique entre deux textes.
        
        Args:
            text1: Premier texte
            text2: DeuxiÃ¨me texte
        
        Returns:
            float: Score de similaritÃ© entre -1 et 1
                  (1 = identique, 0 = non corrÃ©lÃ©, -1 = opposÃ©)
        
        Note:
            Utilise la similaritÃ© cosinus :
            cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
        
        Example:
            >>> embedder = Embedder()
            >>> 
            >>> sim1 = embedder.similarity("cat", "chat")
            >>> print(f"cat vs chat: {sim1:.2f}")
            cat vs chat: 0.65  # Assez similaire
            >>> 
            >>> sim2 = embedder.similarity("cat", "computer")
            >>> print(f"cat vs computer: {sim2:.2f}")
            cat vs computer: 0.05  # TrÃ¨s diffÃ©rent
        """
        # Encoder les deux textes
        emb1 = self.encode(text1)
        emb2 = self.encode(text2)
        
        # Calculer la similaritÃ© cosinus
        # Note : Les embeddings sont dÃ©jÃ  normalisÃ©s, donc c'est juste un dot product
        similarity_score = np.dot(emb1, emb2)
        
        return float(similarity_score)
    
    def batch_similarity(self, query: str, candidates: List[str]) -> List[float]:
        """
        Calculer la similaritÃ© entre une requÃªte et plusieurs candidats.
        
        Args:
            query: Texte de rÃ©fÃ©rence
            candidates: Liste de textes Ã  comparer
        
        Returns:
            List[float]: Scores de similaritÃ© pour chaque candidat
        
        Example:
            >>> embedder = Embedder()
            >>> scores = embedder.batch_similarity(
            ...     "DAG for S3",
            ...     ["S3 pipeline", "API scraper", "Database backup"]
            ... )
            >>> print(scores)
            [0.85, 0.32, 0.41]  # S3 pipeline est le plus similaire
        """
        # Encoder la requÃªte
        query_emb = self.encode(query)
        
        # Encoder tous les candidats
        candidate_embs = self.encode(candidates)
        
        # Calculer les similaritÃ©s (dot product car normalisÃ©)
        similarities = np.dot(candidate_embs, query_emb)
        
        return similarities.tolist()
    
    def get_info(self) -> dict:
        """
        Obtenir des informations sur le modÃ¨le.
        
        Returns:
            dict: Informations (nom, dimensions, etc.)
        """
        return {
            "model_name": self.model_name,
            "embedding_dimension": self.embedding_dimension,
            "max_seq_length": self.model.max_seq_length,
        }


# === Fonction utilitaire pour crÃ©er un embedder global ===

_global_embedder = None

def get_embedder() -> Embedder:
    """
    Singleton pour obtenir une instance unique d'Embedder.
    
    Ã‰vite de recharger le modÃ¨le plusieurs fois (Ã©conomie de RAM).
    
    Returns:
        Embedder: Instance partagÃ©e de l'embedder
    
    Example:
        >>> from agent_ia.rag.embedder import get_embedder
        >>> embedder = get_embedder()  # Charge le modÃ¨le
        >>> embedder2 = get_embedder()  # RÃ©utilise la mÃªme instance
        >>> embedder is embedder2
        True
    """
    global _global_embedder
    
    if _global_embedder is None:
        _global_embedder = Embedder()
    
    return _global_embedder
"""
Indexation des DAG Airflow existants dans ChromaDB.

Ce module :
1. Lit les fichiers .py des DAG
2. Extrait les m√©tadonn√©es avec AST (dag_id, tasks, imports, etc.)
3. Cr√©e des embeddings du code et de la description
4. Stocke dans ChromaDB pour la recherche s√©mantique
"""

import os
import ast
from pathlib import Path
from typing import Dict, List, Optional, Any
import chromadb
from chromadb.config import Settings

from .embedder import get_embedder


class DAGIndexer:
    """
    Indexeur de DAG Airflow pour le RAG.
    
    Permet de cr√©er une base de connaissance searchable
    √† partir de DAG existants.
    """
    
    def __init__(self, db_path: str = "rag_database"):
        """
        Initialiser l'indexeur.
        
        Args:
            db_path: Chemin vers la base de donn√©es ChromaDB
        """
        self.db_path = db_path
        self.embedder = get_embedder()
        
        print(f"üìä Initialisation de la base RAG : {db_path}")
        
        # Cr√©er le client ChromaDB
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,  # D√©sactiver la t√©l√©m√©trie
                allow_reset=True
            )
        )
        
        # Cr√©er ou r√©cup√©rer la collection
        self.collection = self.client.get_or_create_collection(
            name="airflow_dags",
            metadata={
                "description": "Collection of Airflow DAG examples for RAG",
                "embedding_model": "all-MiniLM-L6-v2"
            }
        )
        
        print(f"‚úÖ Collection initialis√©e : {self.collection.count()} DAG d√©j√† index√©s")
    
    def extract_dag_metadata(self, filepath: str) -> Optional[Dict[str, Any]]:
        """
        Extraire les m√©tadonn√©es d'un fichier DAG Python.
        
        Utilise l'AST pour analyser le code sans l'ex√©cuter.
        
        Args:
            filepath: Chemin vers le fichier .py
        
        Returns:
            dict | None: M√©tadonn√©es extraites ou None si erreur
        """
        try:
            # Lire le fichier
            with open(filepath, 'r', encoding='utf-8') as f:
                code = f.read()
            
            # Parser avec AST
            tree = ast.parse(code, filename=filepath)
            
            # Structure de m√©tadonn√©es
            metadata = {
                'filepath': filepath,
                'filename': Path(filepath).name,
                'code': code,
                'code_length': len(code),
                'dag_id': None,
                'description': None,
                'schedule_interval': None,
                'tags': [],
                'task_ids': [],
                'imports': [],
                'operators': [],
                'functions': [],
                'has_taskflow': False,
            }
            
            # Extraire le docstring du module
            docstring = ast.get_docstring(tree)
            if docstring:
                # Garder seulement les 500 premiers caract√®res
                metadata['description'] = docstring[:500].strip()
            
            # Parcourir l'arbre AST
            for node in ast.walk(tree):
                
                # === IMPORTS ===
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        metadata['imports'].append(alias.name)
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        metadata['imports'].append(node.module)
                        
                        # D√©tecter les op√©rateurs utilis√©s
                        for alias in node.names:
                            if 'Operator' in alias.name:
                                metadata['operators'].append(alias.name)
                
                # === FONCTIONS ===
                elif isinstance(node, ast.FunctionDef):
                    metadata['functions'].append(node.name)
                    
                    # D√©tecter TaskFlow API (d√©corateurs @task)
                    for decorator in node.decorator_list:
                        if isinstance(decorator, ast.Name) and decorator.id == 'task':
                            metadata['has_taskflow'] = True
                        elif isinstance(decorator, ast.Attribute) and decorator.attr == 'task':
                            metadata['has_taskflow'] = True
                
                # === ARGUMENTS KEYWORD (dag_id, task_id, etc.) ===
                elif isinstance(node, ast.keyword):
                    
                    # dag_id
                    if node.arg == 'dag_id':
                        if isinstance(node.value, ast.Constant):
                            metadata['dag_id'] = node.value.value
                    
                    # task_id
                    elif node.arg == 'task_id':
                        if isinstance(node.value, ast.Constant):
                            metadata['task_ids'].append(node.value.value)
                    
                    # schedule_interval
                    elif node.arg == 'schedule_interval' or node.arg == 'schedule':
                        if isinstance(node.value, ast.Constant):
                            metadata['schedule_interval'] = node.value.value
                    
                    # tags
                    elif node.arg == 'tags':
                        if isinstance(node.value, ast.List):
                            for elt in node.value.elts:
                                if isinstance(elt, ast.Constant):
                                    metadata['tags'].append(elt.value)
            
            # Si pas de dag_id trouv√©, utiliser le nom du fichier
            if not metadata['dag_id']:
                metadata['dag_id'] = Path(filepath).stem
            
            # D√©dupliquer les listes
            metadata['imports'] = list(set(metadata['imports']))
            metadata['operators'] = list(set(metadata['operators']))
            metadata['task_ids'] = list(set(metadata['task_ids']))
            
            return metadata
        
        except SyntaxError as e:
            print(f"‚ö†Ô∏è  Erreur de syntaxe dans {filepath}: {e}")
            return None
        
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de l'analyse de {filepath}: {e}")
            return None
    
    def create_searchable_text(self, metadata: Dict[str, Any]) -> str:
        """
        Cr√©er un texte optimis√© pour la recherche s√©mantique.
        
        Ce texte sera converti en embedding et stock√© dans ChromaDB.
        
        Args:
            metadata: M√©tadonn√©es extraites du DAG
        
        Returns:
            str: Texte descriptif du DAG
        """
        parts = []
        
        # DAG ID
        if metadata['dag_id']:
            parts.append(f"DAG ID: {metadata['dag_id']}")
        
        # Description
        if metadata['description']:
            parts.append(f"Description: {metadata['description']}")
        
        # Schedule
        if metadata['schedule_interval']:
            parts.append(f"Schedule: {metadata['schedule_interval']}")
        
        # Tags
        if metadata['tags']:
            parts.append(f"Tags: {', '.join(metadata['tags'])}")
        
        # Tasks
        if metadata['task_ids']:
            parts.append(f"Tasks: {', '.join(metadata['task_ids'][:10])}")  # Max 10
        
        # Operators utilis√©s
        if metadata['operators']:
            parts.append(f"Operators: {', '.join(metadata['operators'])}")
        
        # Pattern TaskFlow
        if metadata['has_taskflow']:
            parts.append("Uses TaskFlow API with @task decorators")
        
        # Imports importants (filtrer le bruit)
        relevant_imports = [
            imp for imp in metadata['imports']
            if any(keyword in imp for keyword in ['airflow', 'aws', 's3', 'postgres', 'mysql', 'http', 'email'])
        ]
        if relevant_imports:
            parts.append(f"Imports: {', '.join(relevant_imports[:5])}")
        
        return "\n".join(parts)
    
    def index_dag(self, filepath: str, overwrite: bool = False) -> bool:
        """
        Indexer un seul fichier DAG.
        
        Args:
            filepath: Chemin vers le fichier .py
            overwrite: R√©indexer m√™me si d√©j√† pr√©sent
        
        Returns:
            bool: True si succ√®s, False sinon
        """
        filename = Path(filepath).name
        
        # V√©rifier si d√©j√† index√©
        if not overwrite:
            existing = self.collection.get(ids=[filename])
            if existing['ids']:
                print(f"‚è≠Ô∏è  {filename} d√©j√† index√© (utilisez overwrite=True pour r√©indexer)")
                return True
        
        # Extraire les m√©tadonn√©es
        metadata = self.extract_dag_metadata(filepath)
        
        if not metadata:
            print(f"‚ùå Impossible d'extraire les m√©tadonn√©es de {filename}")
            return False
        
        # Cr√©er le texte searchable
        searchable_text = self.create_searchable_text(metadata)
        
        # G√©n√©rer l'embedding
        embedding = self.embedder.encode(searchable_text)
        
        # Pr√©parer les m√©tadonn√©es pour ChromaDB (seulement des types simples)
        chroma_metadata = {
            'filename': metadata['filename'],
            'dag_id': metadata['dag_id'] or 'unknown',
            'description': (metadata['description'] or '')[:200],  # Limiter la taille
            'schedule': str(metadata['schedule_interval'] or 'None'),
            'num_tasks': len(metadata['task_ids']),
            'has_taskflow': metadata['has_taskflow'],
            'tags': ','.join(metadata['tags'][:5]),  # Max 5 tags
            'operators': ','.join(metadata['operators'][:5]),
        }
        
        # Ajouter ou mettre √† jour dans ChromaDB
        try:
            self.collection.upsert(
                ids=[filename],
                embeddings=[embedding.tolist()],
                documents=[metadata['code']],  # Code complet
                metadatas=[chroma_metadata]
            )
            
            print(f"‚úÖ Index√© : {filename} (DAG: {metadata['dag_id']}, {len(metadata['task_ids'])} t√¢ches)")
            return True
        
        except Exception as e:
            print(f"‚ùå Erreur lors de l'indexation de {filename}: {e}")
            return False
    
    def index_directory(self, directory: str, recursive: bool = False, overwrite: bool = False) -> Dict[str, int]:
        """
        Indexer tous les DAG d'un r√©pertoire.
        
        Args:
            directory: Chemin vers le dossier
            recursive: Inclure les sous-dossiers
            overwrite: R√©indexer les DAG d√©j√† pr√©sents
        
        Returns:
            dict: Statistiques (success, failed, skipped)
        """
        if not os.path.exists(directory):
            print(f"‚ö†Ô∏è  Le dossier {directory} n'existe pas")
            return {'success': 0, 'failed': 0, 'skipped': 0}
        
        print(f"\nüìÅ Indexation des DAG dans : {directory}")
        print(f"   R√©cursif : {recursive}")
        print(f"   √âcraser existants : {overwrite}")
        print()
        
        stats = {'success': 0, 'failed': 0, 'skipped': 0}
        
        # Trouver tous les fichiers .py
        if recursive:
            py_files = list(Path(directory).rglob('*.py'))
        else:
            py_files = list(Path(directory).glob('*.py'))
        
        # Filtrer les fichiers sp√©ciaux
        py_files = [
            f for f in py_files
            if not f.name.startswith(('_', '.'))
            and f.name != '__init__.py'
        ]
        
        print(f"üîç {len(py_files)} fichiers Python trouv√©s")
        print()
        
        # Indexer chaque fichier
        for filepath in py_files:
            result = self.index_dag(str(filepath), overwrite=overwrite)
            
            if result:
                stats['success'] += 1
            else:
                stats['failed'] += 1
        
        # Afficher le r√©sum√©
        print()
        print("="*60)
        print(f"üìä R√âSUM√â DE L'INDEXATION")
        print("="*60)
        print(f"‚úÖ Succ√®s : {stats['success']}")
        print(f"‚ùå √âchecs : {stats['failed']}")
        print(f"üì¶ Total dans la base : {self.collection.count()} DAG")
        print("="*60)
        
        return stats
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Obtenir des statistiques sur la base RAG.
        
        Returns:
            dict: Statistiques d√©taill√©es
        """
        count = self.collection.count()
        
        stats = {
            'total_dags': count,
            'collection_name': self.collection.name,
            'db_path': self.db_path,
        }
        
        # R√©cup√©rer tous les DAG pour des stats d√©taill√©es
        if count > 0:
            all_data = self.collection.get()
            
            # Compter les op√©rateurs utilis√©s
            operators = {}
            for metadata in all_data['metadatas']:
                for op in metadata.get('operators', '').split(','):
                    op = op.strip()
                    if op:
                        operators[op] = operators.get(op, 0) + 1
            
            stats['top_operators'] = dict(sorted(operators.items(), key=lambda x: x[1], reverse=True)[:5])
            stats['has_taskflow_count'] = sum(1 for m in all_data['metadatas'] if m.get('has_taskflow'))
        
        return stats
    
    def reset_collection(self):
        """
        R√©initialiser compl√®tement la collection (DANGER : supprime tout).
        """
        print("‚ö†Ô∏è  ATTENTION : Suppression de toute la collection...")
        self.client.delete_collection(name="airflow_dags")
        self.collection = self.client.create_collection(name="airflow_dags")
        print("‚úÖ Collection r√©initialis√©e")
"""
DAG de synchronisation PostgreSQL â†’ MySQL.

Ce DAG illustre :
- Connexions Ã  plusieurs bases de donnÃ©es
- Mapping de schÃ©mas diffÃ©rents
- Gestion des transactions
- Pattern de synchronisation incrÃ©mentale
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.mysql.hooks.mysql import MySqlHook
from airflow.operators.python import PythonOperator
import pandas as pd


def extract_from_postgres(**context):
    """
    Extraire les donnÃ©es depuis PostgreSQL.
    
    Extraction incrÃ©mentale basÃ©e sur last_updated.
    """
    print("ðŸ“¥ Extraction depuis PostgreSQL...")
    
    # RÃ©cupÃ©rer la derniÃ¨re date de sync depuis XCom (ou dÃ©faut)
    ti = context['ti']
    last_sync = ti.xcom_pull(
        task_ids='extract_from_postgres',
        key='last_sync_date',
        default='2024-01-01'
    )
    
    print(f"ðŸ” DerniÃ¨re sync : {last_sync}")
    
    # Connexion PostgreSQL
    pg_hook = PostgresHook(postgres_conn_id='postgres_source')
    
    # RequÃªte incrÃ©mentale
    query = f"""
        SELECT 
            id,
            user_id,
            product_name,
            quantity,
            price,
            order_date,
            updated_at
        FROM orders
        WHERE updated_at > '{last_sync}'
        ORDER BY updated_at ASC
        LIMIT 10000;
    """
    
    # ExÃ©cuter la requÃªte
    df = pg_hook.get_pandas_df(query)
    
    print(f"âœ… {len(df)} nouvelles lignes extraites")
    
    if len(df) == 0:
        print("â„¹ï¸  Aucune nouvelle donnÃ©e")
        return None
    
    # Sauvegarder temporairement
    temp_path = f"/tmp/pg_data_{context['ds_nodash']}.csv"
    df.to_csv(temp_path, index=False)
    
    # RÃ©cupÃ©rer la derniÃ¨re date pour la prochaine sync
    latest_date = df['updated_at'].max()
    
    ti.xcom_push(key='temp_csv_path', value=temp_path)
    ti.xcom_push(key='last_sync_date', value=str(latest_date))
    ti.xcom_push(key='row_count', value=len(df))
    
    return temp_path


def transform_for_mysql(**context):
    """
    Transformer les donnÃ©es pour le schÃ©ma MySQL.
    
    Mapping de colonnes et conversion de types.
    """
    ti = context['ti']
    temp_path = ti.xcom_pull(task_ids='extract_from_postgres', key='temp_csv_path')
    
    if not temp_path:
        print("â„¹ï¸  Aucune donnÃ©e Ã  transformer")
        return None
    
    print(f"ðŸ”§ Transformation pour MySQL depuis {temp_path}")
    
    # Lire les donnÃ©es
    df = pd.read_csv(temp_path)
    
    # Mapping de colonnes (schÃ©ma PostgreSQL â†’ MySQL)
    column_mapping = {
        'id': 'order_id',
        'user_id': 'customer_id',
        'product_name': 'product',
        'quantity': 'qty',
        'price': 'unit_price',
        'order_date': 'created_date',
        'updated_at': 'sync_date'
    }
    
    df = df.rename(columns=column_mapping)
    
    # Ajouter une colonne calculÃ©e
    df['total_amount'] = df['qty'] * df['unit_price']
    
    # Conversion de dates
    df['created_date'] = pd.to_datetime(df['created_date']).dt.strftime('%Y-%m-%d')
    df['sync_date'] = pd.to_datetime(df['sync_date']).dt.strftime('%Y-%m-%d %H:%M:%S')
    
    print(f"âœ… {len(df)} lignes transformÃ©es")
    print(f"ðŸ“Š Colonnes finales : {df.columns.tolist()}")
    
    # Sauvegarder
    transformed_path = temp_path.replace('.csv', '_transformed.csv')
    df.to_csv(transformed_path, index=False)
    
    ti.xcom_push(key='transformed_csv_path', value=transformed_path)
    
    return transformed_path


def load_to_mysql(**context):
    """
    Charger les donnÃ©es dans MySQL.
    
    Utilise INSERT ... ON DUPLICATE KEY UPDATE.
    """
    ti = context['ti']
    transformed_path = ti.xcom_pull(task_ids='transform_data', key='transformed_csv_path')
    
    if not transformed_path:
        print("â„¹ï¸  Aucune donnÃ©e Ã  charger")
        return 0
    
    print(f"ðŸ’¾ Chargement dans MySQL depuis {transformed_path}")
    
    # Lire les donnÃ©es transformÃ©es
    df = pd.read_csv(transformed_path)
    
    # Connexion MySQL
    mysql_hook = MySqlHook(mysql_conn_id='mysql_target')
    
    # PrÃ©parer les donnÃ©es pour l'insertion
    records = df.to_dict('records')
    
    # Construire la requÃªte INSERT avec UPSERT
    columns = ', '.join(df.columns)
    placeholders = ', '.join(['%s'] * len(df.columns))
    update_clause = ', '.join([f"{col}=VALUES({col})" for col in df.columns if col != 'order_id'])
    
    insert_query = f"""
        INSERT INTO orders_sync ({columns})
        VALUES ({placeholders})
        ON DUPLICATE KEY UPDATE {update_clause};
    """
    
    # Insertion par batch
    batch_size = 500
    total_inserted = 0
    
    conn = mysql_hook.get_conn()
    cursor = conn.cursor()
    
    try:
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            
            # PrÃ©parer les valeurs
            values = [tuple(record.values()) for record in batch]
            
            # ExÃ©cuter le batch
            cursor.executemany(insert_query, values)
            conn.commit()
            
            total_inserted += len(batch)
            print(f"  âœ… {total_inserted}/{len(records)} lignes insÃ©rÃ©es")
        
        print(f"âœ… Chargement terminÃ© : {total_inserted} lignes")
        
        return total_inserted
    
    except Exception as e:
        conn.rollback()
        print(f"âŒ Erreur lors du chargement : {e}")
        raise
    
    finally:
        cursor.close()
        conn.close()


def verify_sync(**context):
    """
    VÃ©rifier l'intÃ©gritÃ© de la synchronisation.
    
    Compare les comptages entre PostgreSQL et MySQL.
    """
    print("ðŸ” VÃ©rification de l'intÃ©gritÃ©...")
    
    # Comptage PostgreSQL
    pg_hook = PostgresHook(postgres_conn_id='postgres_source')
    pg_count = pg_hook.get_first("SELECT COUNT(*) FROM orders;")[0]
    
    # Comptage MySQL
    mysql_hook = MySqlHook(mysql_conn_id='mysql_target')
    mysql_count = mysql_hook.get_first("SELECT COUNT(*) FROM orders_sync;")[0]
    
    print(f"ðŸ“Š PostgreSQL : {pg_count} lignes")
    print(f"ðŸ“Š MySQL : {mysql_count} lignes")
    
    # TolÃ©rance de 1% de diffÃ©rence
    diff_pct = abs(pg_count - mysql_count) / pg_count * 100 if pg_count > 0 else 0
    
    if diff_pct <= 1.0:
        print(f"âœ… Synchronisation OK (diffÃ©rence : {diff_pct:.2f}%)")
        return True
    else:
        print(f"âš ï¸  Ã‰cart important : {diff_pct:.2f}%")
        # En production : envoyer une alerte
        return False


# === DÃ©finition du DAG ===

default_args = {
    'owner': 'data_engineering',
    'depends_on_past': True,  # Ne pas lancer si la run prÃ©cÃ©dente a Ã©chouÃ©
    'email': ['data-eng@company.com'],
    'email_on_failure': True,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'execution_timeout': timedelta(hours=2),
}

with DAG(
    dag_id='postgres_to_mysql_sync',
    default_args=default_args,
    description='Incremental sync from PostgreSQL to MySQL with schema mapping',
    schedule_interval='0 */6 * * *',  # Toutes les 6 heures
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['sync', 'postgres', 'mysql', 'etl'],
    max_active_runs=1,
) as dag:
    
    # TÃ¢che 1 : Extraction
    extract = PythonOperator(
        task_id='extract_from_postgres',
        python_callable=extract_from_postgres,
    )
    
    # TÃ¢che 2 : Transformation
    transform = PythonOperator(
        task_id='transform_data',
        python_callable=transform_for_mysql,
    )
    
    # TÃ¢che 3 : Chargement
    load = PythonOperator(
        task_id='load_to_mysql',
        python_callable=load_to_mysql,
    )
    
    # TÃ¢che 4 : VÃ©rification
    verify = PythonOperator(
        task_id='verify_sync',
        python_callable=verify_sync,
    )
    
    # DÃ©pendances
    extract >> transform >> load >> verify
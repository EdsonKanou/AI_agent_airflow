"""
DAG Ã©ducatif : Chaque ligne est expliquÃ©e.
Ce DAG simule un pipeline ETL (Extract, Transform, Load).
"""

# ========================================
# 1. IMPORTS
# ========================================
from datetime import datetime, timedelta  # Gestion des dates
from airflow import DAG  # Classe principale
from airflow.operators.python import PythonOperator  # ExÃ©cuter du Python
from airflow.operators.bash import BashOperator  # ExÃ©cuter des commandes shell
from airflow.utils.dates import days_ago  # Utilitaire pour dates relatives


# ========================================
# 2. FONCTIONS MÃ‰TIER (ce que font les tÃ¢ches)
# ========================================

def extraire_donnees():
    """
    EXTRACT : Simuler l'extraction de donnÃ©es depuis une source.
    Dans la vraie vie : requÃªte API, lecture fichier, connexion BDD.
    """
    print("ðŸ“¥ Extraction des donnÃ©es depuis la source...")
    donnees = {"users": 150, "transactions": 3200}
    print(f"DonnÃ©es extraites : {donnees}")
    return donnees  # âš ï¸ Les return sont stockÃ©s par Airflow (XCom)


def transformer_donnees(**context):
    """
    TRANSFORM : Nettoyer, formater, agrÃ©ger les donnÃ©es.
    
    **context : Airflow passe automatiquement des mÃ©tadonnÃ©es
    (task_instance, execution_date, etc.)
    """
    # RÃ©cupÃ©rer le rÃ©sultat de la tÃ¢che prÃ©cÃ©dente
    ti = context['ti']  # ti = Task Instance
    donnees = ti.xcom_pull(task_ids='extraire')  # RÃ©cupÃ©rer depuis XCom
    
    print(f"ðŸ”§ Transformation des donnÃ©es : {donnees}")
    
    # Simulation de transformation
    donnees_transformees = {
        "users_actifs": donnees["users"] * 0.7,
        "revenu_moyen": donnees["transactions"] / donnees["users"]
    }
    
    print(f"DonnÃ©es transformÃ©es : {donnees_transformees}")
    return donnees_transformees


def charger_donnees(**context):
    """
    LOAD : Sauvegarder les donnÃ©es traitÃ©es.
    Dans la vraie vie : INSERT dans BDD, Ã©criture fichier, envoi vers data warehouse.
    """
    ti = context['ti']
    donnees = ti.xcom_pull(task_ids='transformer')
    
    print(f"ðŸ’¾ Chargement des donnÃ©es : {donnees}")
    print("âœ… DonnÃ©es sauvegardÃ©es avec succÃ¨s !")


def envoyer_rapport():
    """
    TÃ¢che finale : notification/rapport.
    """
    print("ðŸ“§ Envoi du rapport aux parties prenantes...")
    print("âœ… Pipeline ETL terminÃ© avec succÃ¨s !")


# ========================================
# 3. DEFAULT_ARGS (Configuration par dÃ©faut)
# ========================================

default_args = {
    # PropriÃ©taire du DAG (pour le monitoring)
    'owner': 'data_team',
    
    # Si une tÃ¢che Ã©choue, ne pas bloquer les tÃ¢ches futures indÃ©pendantes
    'depends_on_past': False,
    
    # Email en cas d'Ã©chec (nÃ©cessite config SMTP dans Airflow)
    'email': ['admin@exemple.com'],
    'email_on_failure': False,  # DÃ©sactivÃ© pour tests locaux
    'email_on_retry': False,
    
    # Nombre de tentatives avant d'abandonner
    'retries': 2,
    
    # DÃ©lai entre chaque tentative
    'retry_delay': timedelta(minutes=2),
    
    # Timeout : annuler la tÃ¢che si elle prend trop de temps
    'execution_timeout': timedelta(minutes=10),
}


# ========================================
# 4. DÃ‰FINITION DU DAG
# ========================================

with DAG(
    # --- Identifiant unique ---
    dag_id='pipeline_etl_complet',
    
    # --- Arguments par dÃ©faut (hÃ©ritÃ©s par toutes les tÃ¢ches) ---
    default_args=default_args,
    
    # --- Description (visible dans l'UI) ---
    description='Pipeline ETL complet : Extract â†’ Transform â†’ Load',
    
    # --- Date de dÃ©but ---
    # Le DAG peut s'exÃ©cuter Ã  partir de cette date
    start_date=datetime(2024, 1, 1),
    
    # --- Planification ---
    # Cron: "0 9 * * *" = Tous les jours Ã  9h
    # None = Manuel uniquement
    # @daily, @hourly, @weekly sont des raccourcis
    schedule_interval='0 9 * * *',  # Tous les jours Ã  9h
    
    # --- Catchup ---
    # False = Ne pas exÃ©cuter les runs manquÃ©s
    # True = Rattraper toutes les exÃ©cutions depuis start_date
    catchup=False,
    
    # --- Tags (pour filtrer dans l'UI) ---
    tags=['etl', 'production', 'jour2'],
    
    # --- Timeout global du DAG ---
    dagrun_timeout=timedelta(hours=1),
    
) as dag:
    
    # ========================================
    # 5. DÃ‰FINITION DES TÃ‚CHES
    # ========================================
    
    # --- TÃ¢che 1 : Extraction ---
    tache_extraction = PythonOperator(
        task_id='extraire',  # ID unique dans le DAG
        python_callable=extraire_donnees,  # Fonction Ã  exÃ©cuter
        # provide_context=True,  # ObsolÃ¨te en Airflow 2.x
    )
    
    # --- TÃ¢che 2 : Transformation ---
    tache_transformation = PythonOperator(
        task_id='transformer',
        python_callable=transformer_donnees,
        # Fournir le contexte Airflow (task_instance, etc.)
        provide_context=True,
    )
    
    # --- TÃ¢che 3 : Chargement ---
    tache_chargement = PythonOperator(
        task_id='charger',
        python_callable=charger_donnees,
        provide_context=True,
    )
    
    # --- TÃ¢che 4 : Nettoyage (commande shell) ---
    tache_nettoyage = BashOperator(
        task_id='nettoyer_fichiers_temp',
        bash_command='echo "ðŸ—‘ï¸  Nettoyage des fichiers temporaires..." && ls -la',
    )
    
    # --- TÃ¢che 5 : Rapport ---
    tache_rapport = PythonOperator(
        task_id='envoyer_rapport',
        python_callable=envoyer_rapport,
    )
    
    # ========================================
    # 6. DÃ‰PENDANCES (Ordre d'exÃ©cution)
    # ========================================
    
    # Syntaxe 1 : ChaÃ®nage linÃ©aire
    tache_extraction >> tache_transformation >> tache_chargement
    
    # Syntaxe 2 : ParallÃ©lisation
    # AprÃ¨s le chargement, nettoyage ET rapport en parallÃ¨le
    tache_chargement >> [tache_nettoyage, tache_rapport]
    
    # Ã‰quivalent Ã  :
    # tache_chargement >> tache_nettoyage
    # tache_chargement >> tache_rapport